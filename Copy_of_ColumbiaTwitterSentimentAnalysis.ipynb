{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ColumbiaTwitterSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nivedha75/Columbia-Summer-Coursework/blob/master/Copy_of_ColumbiaTwitterSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZhXXmBr2ZDk",
        "colab_type": "text"
      },
      "source": [
        "# Classes and Functions\n",
        "\n",
        "\n",
        "\n",
        "*   PreProcessTweets Class\n",
        "*   Build Vocabulary method: create a list of all words in training set\n",
        "\n",
        "*   Training method\n",
        "*   Extract Features method: determine if the words in the testing data are in the 'bag of words' of the training set\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgRZRvP8dl2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation \n",
        "from nltk.corpus import stopwords \n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "class PreProcessTweets:\n",
        "    def __init__(self):\n",
        "        self.__stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
        "        \n",
        "    def processTweets(self, dataset):\n",
        "        processedTweets=[]\n",
        "        #df is passed in as a dataset\n",
        "        #each tweet is a row in the dataset and 'tweet' in the for loop is the index of the row\n",
        "        #range(len(dataset)) returns all indices of the dataset\n",
        "        #thus we are going row by row, or tweet by tweet in the dataset\n",
        "        for tweet in range(len(dataset)):\n",
        "            #add a processed tweet represented by (['word', 'word'], label_integer) to the list processedTweets\n",
        "            #dataset.iloc[tweet][1] is the text from the tweet from dataset\n",
        "            #(self.processTweet(dataset.iloc[tweet][1]) is the processed text from the tweet\n",
        "            #dataset.iloc[tweet][0] is the label 0 or 1 of the tweet\n",
        "            processedTweets.append((self.processTweet(dataset.iloc[tweet][1]),dataset.iloc[tweet][0]))\n",
        "        return processedTweets\n",
        "    \n",
        "    def processTweet(self, tweet):\n",
        "        # convert text to lower-case\n",
        "        tweet = tweet.lower()\n",
        "        # remove URLs\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet)\n",
        "        # remove usernames\n",
        "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet)\n",
        "        # remove the # in #hashtag\n",
        "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
        "        # remove repeated characters (helloooooooo into hello)\n",
        "        tweet = word_tokenize(tweet)\n",
        "        #returns a tuple with only necessary words\n",
        "        return [word for word in tweet if word not in self.__stopwords]\n",
        "\n",
        "#get the dataset and preprocess it\n",
        "df = pd.read_csv('20000sample16M.csv')\n",
        "tweetProcessor=PreProcessTweets()\n",
        "NewTrain=tweetProcessor.processTweets(df)\n",
        "\n",
        "def buildVocabulary(preprocessedTrainingData):\n",
        "    all_words = []\n",
        "    for i in range(len(preprocessedTrainingData)):\n",
        "        all_words.extend(preprocessedTrainingData[i][0])\n",
        "    #The wordlist is an instance of FreqDist whose superclass is Counter\n",
        "    #In the constructor, a dictionary is created with the word as the key and the frequency of that word as the value in each key value pair\n",
        "    wordlist = nltk.FreqDist(all_words)\n",
        "    #only the words are returned\n",
        "    word_features = wordlist.keys()\n",
        "    return word_features\n",
        "\n",
        "def train(Train):\n",
        "    trainingFeatures=nltk.classify.apply_features(extract_features,Train)\n",
        "    NBayesClassifier=nltk.NaiveBayesClassifier.train(trainingFeatures)\n",
        "    #print(\"NBayesClassifier: \", NBayesClassifier)\n",
        "    return NBayesClassifier\n",
        "\n",
        "def extract_features(tweet):\n",
        "    tweet_words = set(tweet)\n",
        "    features = {}\n",
        "    #going through each word in the training data\n",
        "    for word in word_features:\n",
        "        #creates a dictionary like this: {'contains(happy)': False}\n",
        "        features['contains(%s)' % word] = (word in tweet_words)\n",
        "    return features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYYqTh804QZa",
        "colab_type": "text"
      },
      "source": [
        "# Execution of Classes/Functions to Train and Test the Model\n",
        " \n",
        "\n",
        "1.   Split the dataset into the training set and testing sets using KFold Cross Validation.\n",
        "\n",
        "2.   Use buildVocabulary and train methods to train the Naive Bayes Classifier model on the training set.\n",
        "\n",
        "3. Use extract_features method to extract features from the tweet and feed that to Naive Bayes Classifier, which will be tested.\n",
        "\n",
        "4. The output, the prediction, will be compared to the real y output (Test_y) through the calculation of f1_score.\n",
        "\n",
        "5. The f1_scores and the overall performance will be printed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJxWuh2fgUi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf5 = KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for train_index, test_index in kf5.split(NewTrain):\n",
        "    #List of tuples, or tweets, for training\n",
        "    Train = [NewTrain[i] for i in train_index]\n",
        "    #List of tuples, or tweets, for testing the NBayes Classifier Model\n",
        "    Test = [NewTrain[i] for i in test_index]\n",
        "    #word_features contains a list of all words in the training data\n",
        "    word_features = buildVocabulary(Train)\n",
        "    NBayesClassifier = train(Train)\n",
        "    #prediction and Test_y are a list containing 4s and 0s\n",
        "    prediction = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in Test]\n",
        "    Test_y = [i[1] for i in Test]\n",
        "    print(f1_score(Test_y,prediction,average='macro'))\n",
        "    sum += float(f1_score(Test_y,prediction,average='macro'))\n",
        "\n",
        "print(\"Overall Performance: \" + str(sum/5))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}